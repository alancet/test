{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ハイパーパラメータ自動調整いろいろ\n",
    "## Kerasと遺伝アルゴリズム\n",
    "\n",
    "https://qiita.com/cvusk/items/1f3b178f34c39beb29ff#_reference-3c5df6e5237c0c705690\n",
    "\n",
    "https://github.com/shibuiwilliam/keras_opt/blob/master/ga_nn.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from keras.layers import Activation, Dropout, BatchNormalization, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import mnist\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST():\n",
    "    def __init__(self, \n",
    "                 l1_out=512, l2_out=512, \n",
    "                 l1_drop=0.2, l2_drop=0.2,\n",
    "                 bn1=0, bn2=0,\n",
    "                 batch_size=100, epochs=10,\n",
    "                 validation_split=0.1):\n",
    "        self.l1_out = l1_out\n",
    "        self.l2_out = l2_out\n",
    "        self.l1_drop = l1_drop\n",
    "        self.l2_drop = l2_drop\n",
    "        self.bn1 = bn1\n",
    "        self.bn2 = bn2\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.validation_split = validation_split\n",
    "        self.__x_train, self.__xtest, self.__y_train, self.__y_test = self.mnist_data()\n",
    "        self.__model = self.mnist_model()\n",
    "        params = \"\"\"\n",
    "        validation_split:\\t{0}\n",
    "        l1_drop:\\t{1}\n",
    "        l2_drop:\\t{2}\n",
    "        l1_out:\\t{3}\n",
    "        l2_out:\\t{4}\n",
    "        bn1:\\t{5}\n",
    "        bn2:\\t{6}\n",
    "        batch_size:\\t{7}\n",
    "        epochs:\\t{8}\n",
    "        \"\"\".format(self.validation_split,\n",
    "                   self.l1_drop, self.l2_drop,\n",
    "                   self.l1_out, self.l2_out,\n",
    "                   self.bn1, self.bn2,\n",
    "                   self.batch_size, self.epochs)\n",
    "        print(params)\n",
    "        \n",
    "    def mnist_data(self):\n",
    "        \"\"\"load mnist data from keras dataset\n",
    "        \"\"\"\n",
    "        (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "        \n",
    "        X_train = X_train.reshape(60000, 784)\n",
    "        X_test = X_test.reshape(10000, 784)\n",
    "        X_train /= 255\n",
    "        X_test /= 255\n",
    "        X_train = X_train.astype('float32')\n",
    "        X_test = X_test.astype('float32')\n",
    "        Y_train = np_utils.to_categorical(y_train, 10)\n",
    "        Y_test = np_utils.to_categorical(y_test, 10)\n",
    "        \n",
    "        return X_train, X_test, Y_train, Y_test\n",
    "    \n",
    "    def mnist_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.l1_out, input_shape=(784,)))\n",
    "        if self.bn1 == 0:\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(self.l1_drop))\n",
    "        \n",
    "        model.add(Dense(self.l2_out))\n",
    "        if self.bn2 == 0:\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(self.l2_drop))\n",
    "        \n",
    "        model.add(Dense(10))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                     optimizer=Adam(),\n",
    "                     metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def mnist_fit(self):\n",
    "        early_stopping = EarlyStopping(patience=0, verbose=1)\n",
    "        \n",
    "        self.__model.fit(self.__x_train, self.__y_train,\n",
    "                        batch_size=self.batch_size,\n",
    "                        epochs=self.epochs,\n",
    "                        verbose=0,\n",
    "                        validation_split=self.validation_split,\n",
    "                        callbacks=[early_stopping])\n",
    "        \n",
    "    def mnist_evaluate(self):\n",
    "        self.mnist_fit()\n",
    "        \n",
    "        evaluation = self.__model.evaluate(self.__x_test,\n",
    "                                          self.__y_test,\n",
    "                                          batch_size,\n",
    "                                          verbose=0)\n",
    "        return evaluation\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (60000, 28, 28) [[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]]\n",
      "y_train (60000,) [5 0 4 1 9 2]\n",
      "(60000, 784)\n",
      "[0 0 0 0 0 0]\n",
      "(60000, 784)\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "(60000, 784)\n",
      "[0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(\"X_train\", X_train.shape, X_train[0, :6, :6])\n",
    "print(\"y_train\", y_train.shape, y_train[:6])\n",
    "\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "print(X_train.shape)\n",
    "print(X_train[0, :6])\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "print(X_train.shape)\n",
    "print(X_train[0, :6])\n",
    "\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape)\n",
    "print(X_train[0, :6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train <class 'numpy.ndarray'> (60000,) 5\n",
      "Y_train <class 'numpy.ndarray'> (60000, 10) [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(\"y_train\", type(y_train), y_train.shape, y_train[0])\n",
    "\n",
    "print(\"Y_train\", type(Y_train), Y_train.shape, Y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mnist(bounds):\n",
    "    _mnist = MNIST(l1_out=bounds[0], \n",
    "                   l2_out=bounds[1], \n",
    "                   l1_drop=bounds[2], \n",
    "                   l2_drop=bounds[3], \n",
    "                   bn1=bounds[4],\n",
    "                   bn2=bounds[5],\n",
    "                   batch_size=bounds[6],\n",
    "                   epochs=bounds[7], \n",
    "                   validation_split=bounds[8])\n",
    "    mnist_evaluation = _mnist.mnist_evaluate()\n",
    "    print(mnist_evaluation)\n",
    "    return mnist_evaluation[0],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alanc\\Anaconda3\\lib\\site-packages\\deap\\tools\\_hypervolume\\pyhv.py:33: ImportWarning: Falling back to the python version of hypervolume module. Expect this to be very slow.\n",
      "  \"module. Expect this to be very slow.\", ImportWarning)\n"
     ]
    }
   ],
   "source": [
    "from deap import base, creator, tools, algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator.create('FitnessMax', base.Fitness, weights = (-1.0, ))\n",
    "creator.create('Individual', list, fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register('l1_out', random.choice, (64, 128, 256, 512, 1024))\n",
    "toolbox.register('l2_out', random.choice, (64, 128, 256, 512, 1024))\n",
    "toolbox.register(\"l1_drop\", random.uniform, 0.0, 0.3)\n",
    "toolbox.register(\"l2_drop\", random.uniform, 0.0, 0.3)\n",
    "toolbox.register(\"bn1\", random.randint, 0, 1)\n",
    "toolbox.register(\"bn2\", random.randint, 0, 1)\n",
    "toolbox.register(\"batch_size\", random.choice, (10, 100, 500))\n",
    "toolbox.register(\"epochs\", random.choice, (5, 10, 20))\n",
    "toolbox.register(\"validation_split\", random.uniform, 0.0, 0.3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
